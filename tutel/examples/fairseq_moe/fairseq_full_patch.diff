diff --git a/fairseq/models/transformer/transformer_decoder.py b/fairseq/models/transformer/transformer_decoder.py
index 61aaa09..458bd40 100644
--- a/fairseq/models/transformer/transformer_decoder.py
+++ b/fairseq/models/transformer/transformer_decoder.py
@@ -115,9 +115,14 @@ class TransformerDecoderBase(FairseqIncrementalDecoder):
             self.layers = LayerDropModuleList(p=self.decoder_layerdrop)
         else:
             self.layers = nn.ModuleList([])
+
+        def config_with_index(cfg, index):
+            cfg.transformer_index = index
+            return cfg
+
         self.layers.extend(
             [
-                self.build_decoder_layer(cfg, no_encoder_attn)
+                self.build_decoder_layer(config_with_index(cfg, _), no_encoder_attn)
                 for _ in range(cfg.decoder.layers)
             ]
         )
diff --git a/fairseq/modules/transformer_layer.py b/fairseq/modules/transformer_layer.py
index 2e687b9..42d56f5 100644
--- a/fairseq/modules/transformer_layer.py
+++ b/fairseq/modules/transformer_layer.py
@@ -324,18 +324,34 @@ class TransformerDecoderLayerBase(nn.Module):
             else None
         )
 
-        self.fc1 = self.build_fc1(
-            self.embed_dim,
-            cfg.decoder.ffn_embed_dim,
-            self.quant_noise,
-            self.quant_noise_block_size,
-        )
-        self.fc2 = self.build_fc2(
-            cfg.decoder.ffn_embed_dim,
-            self.embed_dim,
-            self.quant_noise,
-            self.quant_noise_block_size,
-        )
+        self.moe_freq = int(torch.os.environ.get('MOE', 0))
+        self.use_moe = (self.moe_freq == 0) or (cfg.transformer_index + 1) % self.moe_freq == 0
+        if self.moe_freq == 0:
+            self.use_moe = False
+        if self.use_moe:
+            assert self.quant_noise == 0
+            from tutel.moe import moe_layer
+            self.moe_ffn = moe_layer(
+                gate_type={'type' : 'top', 'k' : 2, 'capacity_factor': 0, 'fp32_gate': True},
+                model_dim=self.embed_dim,
+                experts={'count_per_node': 1,'type': 'ffn', 'hidden_size_per_expert': cfg.decoder.ffn_embed_dim,
+                        'activation_fn' : lambda x:
+                    self.activation_dropout_module(x) if self.ffn_layernorm is None else self.ffn_layernorm(self.activation_dropout_module(x))},
+                scan_expert_func = lambda name, param: setattr(param, 'expert', True),  # This is compatible with Fairseq + legacy_ddp only
+            )
+        else:
+            self.fc1 = self.build_fc1(
+                self.embed_dim,
+                cfg.decoder.ffn_embed_dim,
+                self.quant_noise,
+                self.quant_noise_block_size,
+            )
+            self.fc2 = self.build_fc2(
+                cfg.decoder.ffn_embed_dim,
+                self.embed_dim,
+                self.quant_noise,
+                self.quant_noise_block_size,
+            )
 
         self.final_layer_norm = LayerNorm(self.embed_dim, export=cfg.export)
         self.need_attn = True
@@ -504,11 +520,15 @@ class TransformerDecoderLayerBase(nn.Module):
         if self.normalize_before:
             x = self.final_layer_norm(x)
 
-        x = self.activation_fn(self.fc1(x))
-        x = self.activation_dropout_module(x)
-        if self.ffn_layernorm is not None:
-            x = self.ffn_layernorm(x)
-        x = self.fc2(x)
+        if self.use_moe:
+            x = self.moe_ffn(x)
+        else:
+            x = self.activation_fn(self.fc1(x))
+            x = self.activation_dropout_module(x)
+            if self.ffn_layernorm is not None:
+                x = self.ffn_layernorm(x)
+            x = self.fc2(x)
+
         x = self.dropout_module(x)
         if self.w_resid is not None:
             residual = torch.mul(self.w_resid, residual)
